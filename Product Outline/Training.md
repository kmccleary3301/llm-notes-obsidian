For finetuning, we will utilize training scripts from WizardLM which utilizes the distributed training package [deepspeed](https://www.microsoft.com/en-us/research/blog/deepspeed-zero-a-leap-in-speed-for-llm-and-chat-model-training-with-4x-less-communication/). We will also utilize various training sets from different open-source projects.


I've gone through some effort to do test runs for finetuning LLaMA models with the code from the provided projects. They are linked below.

[[Orca Training]]
[[WizardLM Training]]