> [!faq]- Function Description
> Llama cpp provides fast performance via 4-bit quantization of 
> llama-based models on limited hardware.
> 
> The readme is [available here](https://github.com/ggerganov/llama.cpp).
> 
> The official purpose is for running llama models on macbooks, 
> so I am currently unsure if it will prove useful for the 
> deployment of our LLMs on the A100 clusters.
